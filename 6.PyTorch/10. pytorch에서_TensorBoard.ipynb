{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "epochs = 5\n",
    "no_cuda = False\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths = glob('../dataset/mnist_png/training/*/*.png')\n",
    "test_paths = glob('../dataset/mnist_png/testing/*/*.png')\n",
    "\n",
    "len(train_paths), len(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data_paths[idx]\n",
    "        image = Image.open(path).convert(\"L\")\n",
    "        label = int(path.split('\\\\')[-2])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    Dataset(train_paths, \n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(), \n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize(\n",
    "                    mean=[0.406], \n",
    "                    std=[0.225])])\n",
    "           ),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    Dataset(test_paths,\n",
    "           transforms.Compose([\n",
    "               transforms.ToTensor(), \n",
    "               transforms.Normalize(\n",
    "                   mean=[0.406], \n",
    "                   std=[0.225])])\n",
    "           ),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(data[0].shape, data[1].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()  # logdir을 지정해주지 않아도 자동으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.325451\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.200615\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.120351\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.924122\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.720477\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.264989\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.064239\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.688167\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.943386\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.940783\n",
      "\n",
      "Test set: Average loss: 0.6256, Accuracy: 8087/10000 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.797053\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.634414\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.750086\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.565812\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.562702\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.377767\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.514326\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.357497\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.429412\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.553812\n",
      "\n",
      "Test set: Average loss: 0.4266, Accuracy: 8643/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.322170\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.779165\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.316099\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.475837\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.459780\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.366290\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.232849\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.172914\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.313037\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.373658\n",
      "\n",
      "Test set: Average loss: 0.3395, Accuracy: 8959/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.554159\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.502763\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.254334\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.428004\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.380331\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.692465\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.302111\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.356890\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.259485\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.269908\n",
      "\n",
      "Test set: Average loss: 0.2826, Accuracy: 9135/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.291008\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.290124\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.237676\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.372103\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.136679\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.250020\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.367486\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.172464\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.192212\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.342823\n",
      "\n",
      "Test set: Average loss: 0.2296, Accuracy: 9286/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    # Test mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    \n",
    "    \n",
    "    # Add to TensorBoard\n",
    "    if epoch == 0:\n",
    "        grid = torchvision.utils.make_grid(data)\n",
    "        writer.add_image('images', grid, epoch)\n",
    "        writer.add_graph(model, data)\n",
    "\n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/test', accuracy, epoch)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard 보는 방법\n",
    "- 1.runs 라는 디렉토리로 이동\n",
    "- 2.powershell 실행 (Shift + 마우스 우측 버튼)\n",
    "- 3.tensorboard --logdir=./ --port 8008 (디폴트는 6006 포트 사용)\n",
    "- 4.새브라우저에서 http://localhost:8008 접속"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
