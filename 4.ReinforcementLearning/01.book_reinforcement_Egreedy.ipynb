{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/seungeunrho/RLfrombasics 참조\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = 3 # 1 : E Greedy, 2 : nTD, 3 : Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP\n",
    "  0 1 2 3 4 5 6\n",
    "0 S 0 X 0 0 0 0\n",
    "1 0 0 X 0 0 0 0\n",
    "2 0 0 X 0 X 0 0\n",
    "3 0 0 0 0 X 0 0\n",
    "4 0 0 0 0 X 0 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment class\n",
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        \n",
    "    def step(self, a):\n",
    "        if a == 0:\n",
    "            self.move_left()\n",
    "            #print('left')\n",
    "        elif a == 1:\n",
    "            self.move_right()\n",
    "            #print('right')            \n",
    "        elif a == 2:\n",
    "            self.move_up()\n",
    "            #print('up')            \n",
    "        elif a == 3:\n",
    "            self.move_down()\n",
    "            #print('down')            \n",
    "            \n",
    "        reward = -1\n",
    "        done = self.is_done()\n",
    "        return (self.y, self.x), reward, done\n",
    "    \n",
    "    def move_left(self):\n",
    "        if self.x == 0:\n",
    "            pass\n",
    "        elif self.x == 3 and self.y in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.x == 5 and self.y in [2, 3, 4]:\n",
    "            pass\n",
    "        else:\n",
    "            self.x -= 1        \n",
    "            \n",
    "    def move_right(self):\n",
    "        if self.x == 1 and self.y in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.x == 3 and self.y in [2, 3, 4]:\n",
    "            pass\n",
    "        elif self.x == 6:\n",
    "            pass\n",
    "        else:\n",
    "            self.x += 1        \n",
    "            \n",
    "           \n",
    "    def move_up(self):\n",
    "        if self.y == 0:\n",
    "            pass\n",
    "        elif self.y == 3 and self.x == 2:\n",
    "            pass               \n",
    "        else:\n",
    "            self.y -= 1\n",
    "          \n",
    "    def move_down(self):\n",
    "        if self.y == 4:\n",
    "            pass\n",
    "        elif self.y == 1 and self.x == 4:\n",
    "            pass\n",
    "        else:    \n",
    "            self.y += 1\n",
    "            \n",
    "    def is_done(self):\n",
    "        if self.y == 4 and self.x == 6:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_state(self):\n",
    "        return (self.y, self.x)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.y, self.x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class QAgent():\n",
    "    def __init__(self):\n",
    "        # q value를 저장하는 변수. 모두 0으로 초기화\n",
    "        self.q_table = np.zeros((5,7,4))\n",
    "        self.eps = 0.9\n",
    "        self.alpha = 0.01\n",
    "    \n",
    "    def select_action(self, s):\n",
    "        # eps-greedy로 액션을 선택\n",
    "        y, x = s\n",
    "        coin = random.random()\n",
    "        #print('coin -> ',coin)\n",
    "        \n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            action_val = self.q_table[y, x,:]\n",
    "            # print(y,x,action_val)\n",
    "            # argmax는 가장 큰 값을 가지고 있는 위치\n",
    "            action = np.argmax(action_val)\n",
    "            # print('action -> ', action)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    if METHOD == 1:\n",
    "        def update_table(self, history):\n",
    "            # 한 에피소드에 해당하는 history를 입력으로 받아 q 테이블의 값을 업데이트\n",
    "            cum_reward = 0\n",
    "            for transition in history[::-1]:\n",
    "                s, a, r, s_prime = transition\n",
    "                y, x = s\n",
    "\n",
    "                # MC(몬테카를로)방식을 이용하여 업데이트\n",
    "                #print('transition -> ',transition)\n",
    "                #print('q_table1 -> ',self.q_table[y,x,a])\n",
    "                # MC 수식 Q(S0,A0) = Q(S0,A0) + alpha * (R * gamma의 t-1 제곱 - Q(S0,A0))\n",
    "                self.q_table[y,x,a] = self.q_table[y,x,a] + self.alpha * (cum_reward - self.q_table[y,x,a])\n",
    "                cum_reward = cum_reward + r\n",
    "                #print('q_table2 -> ',self.q_table[y,x,a])\n",
    "                #print('cum_reward -> ',cum_reward) \n",
    "                #print('--------------------------------')    \n",
    "    elif METHOD == 2:\n",
    "        def update_table(self, transition):\n",
    "            s, a, r, s_prime = transition\n",
    "            y, x = s\n",
    "            next_y, next_x = s_prime\n",
    "            \n",
    "            # 벨만 기대 방적식에 따른 진행\n",
    "            # s'에서 선택할 Action이며, 실제로 취한 Action이 아님(정책에 따라 진행)\n",
    "            a_prime = self.select_action(s_prime)\n",
    "            \n",
    "            # SARSA 수식 Q(S0,A0) = Q(S0,A0) + alpha * (R + gamma * Q(S1,A1) - Q(S0,A0))\n",
    "            self.q_table[y,x,a] = self.q_table[y,x,a] + 0.1 * (r + self.q_table[next_y,next_x,a_prime]- self.q_table[y,x,a])        \n",
    "    else:\n",
    "        def update_table(self, transition):\n",
    "            s, a, r, s_prime = transition\n",
    "            y, x = s\n",
    "            next_y, next_x = s_prime\n",
    "            \n",
    "            # 벨만 최적 방적식에 따른 진행\n",
    "            # Q Learning 수식 Q(S0,A0) = Q(S0,A0) + alpha * (R + gamma * MAX(Q(S1,A1)) - Q(S0,A0))\n",
    "            self.q_table[y,x,a] = self.q_table[y,x,a] + 0.1 * (r + np.amax(self.q_table[next_y,next_x,:])- self.q_table[y,x,a])\n",
    "    \n",
    "\n",
    "    def anneal_eps(self):\n",
    "        self.eps -= 0.03\n",
    "        self.eps = max(self.eps, 0.1)\n",
    "        \n",
    "    def show_table(self):\n",
    "        # 학습이 각 위치에서 어느 액션의 q값이 가장 높았는지 보여주는 함수\n",
    "        q_lst = self.q_table.tolist()\n",
    "        data = np.zeros((5,7))\n",
    "        for row_idx in range(len(q_lst)):\n",
    "            row = q_lst[row_idx]\n",
    "            for col_idx in range(len(row)):\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col)\n",
    "                data[row_idx, col_idx] = action\n",
    "        #print(self.q_table)\n",
    "        print('0:left, 1:right, 2:up, 3:down' )\n",
    "        print(data)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = QAgent()\n",
    "\n",
    "    for k in range(1000): # 총 1000번의 에피소드 진행\n",
    "        done = False\n",
    "        history = []\n",
    "        \n",
    "        s = env.reset()\n",
    "        if METHOD == 1:\n",
    "            while not done:\n",
    "                action = agent.select_action(s)\n",
    "                s_prime, reward, done = env.step(action)\n",
    "                #print(s, action, s_prime)\n",
    "                history.append((s, action, reward, s_prime))\n",
    "                s = s_prime\n",
    "\n",
    "            # history를 이용하여 에이전트를 업데이트\n",
    "            agent.update_table(history)\n",
    "        else:\n",
    "            while not done:\n",
    "                action = agent.select_action(s)\n",
    "                s_prime, reward, done = env.step(action)\n",
    "                #print(s, action, s_prime)\n",
    "                agent.update_table((s, action, reward, s_prime))\n",
    "                s = s_prime\n",
    "                \n",
    "        agent.anneal_eps()\n",
    "\n",
    "    # 학습이 끝난 결과를 출력\n",
    "    agent.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:left, 1:right, 2:up, 3:down\n",
      "[[3. 3. 0. 2. 3. 3. 3.]\n",
      " [3. 3. 0. 1. 1. 3. 3.]\n",
      " [1. 3. 0. 2. 0. 1. 3.]\n",
      " [1. 1. 1. 2. 0. 3. 3.]\n",
      " [2. 3. 1. 2. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
