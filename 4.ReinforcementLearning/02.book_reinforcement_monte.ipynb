{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/seungeunrho/RLfrombasics 참조\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC는 에피소드가 종료되어야 리턴을 알 수 있음 (실제 리턴값)\n",
    "# TD는 에피소드가 종료되지 않아도 리턴을 알 수 있음 (추측 리턴값)\n",
    "METHOD = 2 # 1 : MC(Monte Carlo), 2 : TD (Temporal Difference)\n",
    "EPISODE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment class\n",
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        \n",
    "    def step(self, a):\n",
    "        if a == 0:\n",
    "            self.move_right()\n",
    "        elif a == 1:\n",
    "            self.move_left()\n",
    "        elif a == 2:\n",
    "            self.move_up()\n",
    "        elif a == 3:\n",
    "            self.move_down()\n",
    "            \n",
    "        reward = -1\n",
    "        done = self.is_done()\n",
    "        return (self.x, self.y), reward, done\n",
    "    \n",
    "    def move_right(self):\n",
    "        self.y += 1\n",
    "        if self.y > 3:\n",
    "            self.y = 3\n",
    "            \n",
    "    def move_left(self):\n",
    "        self.y -= 1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "            \n",
    "    def move_up(self):\n",
    "        self.x -= 1\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "          \n",
    "    def move_down(self):\n",
    "        self.x += 1\n",
    "        if self.x > 3:\n",
    "            self.x = 3\n",
    "            \n",
    "    def is_done(self):\n",
    "        if self.x == 3 and self.y == 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_state(self):\n",
    "        return (self.x, self.y)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        coin = random.random()\n",
    "        if coin < 0.25:\n",
    "            action = 0\n",
    "        elif coin < 0.5:\n",
    "            action = 1\n",
    "        elif coin < 0.75:\n",
    "            action = 2\n",
    "        else:\n",
    "            action = 3\n",
    "            \n",
    "        return action\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = Agent()\n",
    "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]] # 테이블 초기화\n",
    "    gamma = 1.0\n",
    "    \n",
    "    if METHOD == 1:\n",
    "        alpha = 0.0001\n",
    "        for k in range(EPISODE): # 총 5만번의 에피소드 진행\n",
    "            done = False\n",
    "            history = []\n",
    "            while not done:\n",
    "                action = agent.select_action()\n",
    "                (x,y), reward, done = env.step(action)\n",
    "                history.append((x,y,reward))\n",
    "            env.reset()    \n",
    "            \n",
    "            #print(history)\n",
    "            \n",
    "            # 매 에피소드가 끝나고 바로 해당 데이터를 이용해 테이블을 얻데이트\n",
    "            cum_reward = 0\n",
    "            for transition in history[::-1]:\n",
    "                # 방문했던 상태들을 뒤에서부터 차례차례 리턴을 계산\n",
    "                x, y, reward = transition\n",
    "                #print(transition)\n",
    "\n",
    "                # 최소 수식 V(St) = (1 - alpha) * V(St) + alpha * Gt\n",
    "                # 변경 수식 V(St) = V(St) + alpha * (Gt - V(St))\n",
    "                # V(St)에 대한 값을 근사치에 가까이 가게하기 위하여 EPISODE 많이 해주어야 함\n",
    "                # alpha(0~1 사이 값)의 값이 클수록 경험값이 한번에 크게 업데이트 되고\n",
    "                # alpha의 값이 작을수록 경험값이 조금씩(보수적으로) 업데이트 됨\n",
    "                data[x][y] = data[x][y] + alpha*(cum_reward - data[x][y])\n",
    "\n",
    "                # 수식 Gt = Rt+1 + r * Gt+1\n",
    "                # G0 = R0 + gamma*R1 + .... + gamma의 t-1 제곱 * Rt-1\n",
    "                # Gt-1 = R1\n",
    "                # Gt = 0 -> Gt는 종료 State\n",
    "                # cum_reward는 Gt\n",
    "                # gamma는 감쇠인자. 먼 미래일 경우 보상 값을 적게하기 위함\n",
    "                cum_reward = cum_reward + gamma * reward\n",
    "    else:\n",
    "        alpha = 0.01 # MC보다 큰 값을 사용\n",
    "        for k in range(EPISODE): # 총 5만번의 에피소드 진행\n",
    "            done = False\n",
    "            while not done:\n",
    "                x, y = env.get_state()\n",
    "                action = agent.select_action()\n",
    "                (x_prime, y_prime), reward, done = env.step(action)\n",
    "                #print(x_prime, y_prime)\n",
    "\n",
    "                # V(St)에 대한 값을 근사치에 가까이 가게하기 위하여 EPISODE 많이 해주어야 함\n",
    "                # alpha(0~1 사이 값)의 값이 클수록 경험값이 한번에 크게 업데이트 되고\n",
    "                # alpha의 값이 작을수록 경험값이 조금씩(보수적으로) 업데이트 됨                \n",
    "                # gamma는 감쇠인자. 먼 미래일 경우 보상 값을 적게하기 위함\n",
    "                # 한 번의 step이 진행되자마자 바로 테이블의 데이터를 업데이트 해줌\n",
    "                # 수식 V(St) = V(St) + alpha * (Rt+1 + gamma * V(St+1) - V(St))\n",
    "                data[x][y] = data[x][y] + alpha*(reward + gamma * data[x_prime][y_prime] - data[x][y])\n",
    "\n",
    "            env.reset()    \n",
    "\n",
    "    # 학습이 끝나고 난 후 데이터를 출력해 보기 위한 코드\n",
    "    for row in data:\n",
    "        print(row)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-59.14116007273648, -57.35847726920523, -54.33440637231192, -51.0320059405729]\n",
      "[-57.14057072359945, -54.00313261218074, -48.846343053536444, -44.38458593403774]\n",
      "[-54.02643211956618, -49.61695994391938, -40.68415444793987, -28.839126767513836]\n",
      "[-51.93090658473838, -45.368134627808764, -27.756643685960405, 0]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
