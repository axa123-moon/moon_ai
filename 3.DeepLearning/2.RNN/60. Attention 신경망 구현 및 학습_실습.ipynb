{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention 신경망 구현 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt #pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100 # 초기 설정값은 200이었으나 시간이 많이 소요되어서 100으로 조정함\n",
    "NUM_WORDS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 입력 단어를 embeding 처리함\n",
    "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
    "        \n",
    "        # 입력 단어를 LSTM으로 처리함\n",
    "        # return_sequences=True => 모든 sequence를 돌려줌\n",
    "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.emb(x)\n",
    "        # H는 모든 hidden state\n",
    "        H, h, c = self.lstm(x)\n",
    "        return H, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
    "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
    "        self.att = tf.keras.layers.Attention()\n",
    "        self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        # x : shifted output, s0 : 처음 들어 오는 값, c0 : 처음 들어 오는 값\n",
    "        # H : 인코드에서 나온 모든 hidden 값\n",
    "        x, s0, c0, H = inputs\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # S : 모든 hidden state 값\n",
    "        S, h, c = self.lstm(x, initial_state=[s0, c0])\n",
    "        \n",
    "        S_ = tf.concat([s0[:, tf.newaxis, :], S[:, :-1, :]], axis=1)\n",
    "        \n",
    "        # Attention value (Query, Key, Value)\n",
    "        A = self.att([S_, H])\n",
    "        \n",
    "        # 모든 Hidden state와 Attention value를 구성\n",
    "        y = tf.concat([S, A], axis=-1)\n",
    "        \n",
    "        return self.dense(y), h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(tf.keras.Model):\n",
    "    def __init__(self, sos, eos):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        self.sos = sos # start of sentence\n",
    "        self.eos = eos # end of sentence\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        if training is True:\n",
    "            # x : input, y : output\n",
    "            x, y = inputs\n",
    "            \n",
    "            # h : hidden state, c : cell state\n",
    "            H, h, c = self.enc(x)\n",
    "            \n",
    "            # y : output (전체 문장)\n",
    "            y, _, _ = self.dec((y, h, c, H))\n",
    "            return y\n",
    "        else:\n",
    "            # x : input\n",
    "            x = inputs\n",
    "            \n",
    "            # h : hidden state, c : cell state            \n",
    "            H, h, c = self.enc(x)\n",
    "            \n",
    "            # sos를 tensor type으로 변경\n",
    "            y = tf.convert_to_tensor(self.sos)\n",
    "            \n",
    "            # sos의 shape을 (1) 구조에서 (1,1) 구조로 변경\n",
    "            y = tf.reshape(y, (1, 1))\n",
    "\n",
    "            # 64개 단어로 구성된 문장을 tensor type로 생성\n",
    "            seq = tf.TensorArray(tf.int32, 64)\n",
    "\n",
    "            for idx in tf.range(64):\n",
    "                y, h, c = self.dec([y, h, c, H])\n",
    "                \n",
    "                # tf.argmax(y, axis=-1)는 정답을 추출\n",
    "                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
    "                y = tf.reshape(y, (1, 1))\n",
    "                seq = seq.write(idx, y)\n",
    "\n",
    "                if y == self.eos:\n",
    "                    break\n",
    "\n",
    "            return tf.reshape(seq.stack(), (1, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습, 테스트 루프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement training loop\n",
    "@tf.function\n",
    "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    output_labels = labels[:, 1:] # SOS 를 넣기 위한 구조\n",
    "    shifted_labels = labels[:, :-1] # EOS를 넣기 위한 구조\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([inputs, shifted_labels], training=True)\n",
    "        loss = loss_object(output_labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(output_labels, predictions)\n",
    "\n",
    "# Implement algorithm test\n",
    "@tf.function\n",
    "def test_step(model, inputs):\n",
    "    return model(inputs, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본문장 => ['아이스아메리카노 하나요\\n', '테이크아웃하실 건가요?\\n', '저 카푸치노로 주문할게요\\n', '시럽은 얼마나 뿌려드릴까요?\\n', '저 도장 다 모았는데 나중에 써도 되나요?\\n', '네 다음에 써도 됩니다\\n', '이 기프티콘 여기서 사용할 수 있나요?\\n', '사용 가능하십니다\\n', '자바칩 프라푸치노에 휘핑 빼고요\\n', '6600원입니다\\n']\n",
      "\n",
      "가공된 질문 문장 => ['아이스 아메리카노 하나요 \\n', '저 카푸치노 로 주문 할게요 \\n', '저 도장 다 모았는데 나중 에 써도 되나요 ? \\n', '이 기프티콘 여기 서 사용 할 수 있나요 ? \\n', '자바 칩 프라푸치노 에 휘핑 빼고요 \\n', '여기 기프티콘 대면 되죠 ? \\n', '\" 따뜻한 아메리카노 한 잔 , 아이스 라떼 한 잔이요 .\" \\n', '\" 네 , 그런데 중간 에 테이크 아웃 도 가능한가요 ?\" \\n', '\" 네 , 스콘 도 3 개 같이 주세요 .\" \\n', '카푸치노 차갑게 되나요 ? \\n']\n",
      "\n",
      "가공된 답변 문장 => ['\\t 테이크아웃 하실 건가 요 ? \\n', '\\t 시럽 은 얼마나 뿌려 드릴 까요 ? \\n', '\\t 네 다음 에 써도 됩니다 \\n', '\\t 사용 가능하십니다 \\n', '\\t 6600원 입니다 \\n', '\\t 네 현금영수증 해드릴까 요 ? \\n', '\\t 드시고 가시나요 ? \\n', '\\t \" 네 , 카운터 로 오시 면 테이크 아웃 잔 에 담아 드려요 .\" \\n', '\\t 스콘 도 드시고 가시나요 ? \\n', '\\t 가능합니다 \\n']\n",
      "\n",
      "perm => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "perm => [419, 459, 130, 431, 370, 26, 201, 56, 366, 108]\n",
      "\n",
      "질문 문장=> ['아이스 아메리카노 하나요 \\n', '저 카푸치노 로 주문 할게요 \\n', '저 도장 다 모았는데 나중 에 써도 되나요 ? \\n', '이 기프티콘 여기 서 사용 할 수 있나요 ? \\n', '자바 칩 프라푸치노 에 휘핑 빼고요 \\n', '\" 따뜻한 아메리카노 한 잔 , 아이스 라떼 한 잔이요 .\" \\n', '\" 네 , 스콘 도 3 개 같이 주세요 .\" \\n', '카푸치노 차갑게 되나요 ? \\n', '아이스 카푸치노 개인 컵 에 주실 수 있나요 ? \\n', '음료 와 같이 먹을 빵 추천 해주세요 \\n']\n",
      "\n",
      "질문 토큰 문장=> [[8, 6, 91, 1], [114, 199, 5, 26, 27, 1], [114, 232, 60, 294, 413, 9, 295, 31, 1], [14, 79, 32, 129, 36, 101, 20, 22, 1], [414, 233, 102, 9, 168, 415, 1], [28, 6, 16, 11, 8, 296, 16, 297, 1], [3, 115, 10, 416, 41, 92, 4, 1], [199, 200, 31, 1], [8, 199, 298, 116, 9, 146, 20, 22, 1], [42, 38, 92, 417, 299, 147, 24, 1]]\n",
      "\n",
      "아메리카노 토큰 => 6\n",
      "\n",
      "질문 패딩 문장=> [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   8   6  91   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0 114 199   5  26  27   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 114 232  60 294 413   9 295  31   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  14  79  32 129  36 101  20  22   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0 414 233 102   9 168 415   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  28   6  16  11   8 296  16 297   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   3 115  10 416  41  92   4   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0 199 200  31   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   8 199 298 116   9 146  20  22   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  42  38  92 417 299 147  24   1]]\n",
      "\n",
      "답변 패딩 문장=> [[  2 107 111  97  12   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2  95  17 235 599  13  15   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2   3 600   9 295  67   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2  36 279   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2 601  18   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2  35  59   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2 115  10  35  59   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2  65   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2 298 116  19  23 602  42  23  60  87 603   1   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2 604 154  14 123 605   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "#java JDK 설치 필요\n",
    "dataset_file = '../dataset/chatbot_data.csv' # acquired from 'http://www.aihub.or.kr' and modified\n",
    "okt = Okt()\n",
    "\n",
    "with open(dataset_file, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    # morphs는 형태소 분석 기능\n",
    "    seq = [' '.join(okt.morphs(line)) for line in lines] \n",
    "\n",
    "# 입력데이타는 질문 문장과 답변 문장의 반복으로 구성되어 있음\n",
    "print(\"원본문장 =>\",lines[:10])\n",
    "\n",
    "# 형태소 분석이 완료된 문장을 두칸씩 이동해서 데이타 가져오기\n",
    "questions = seq[::2] \n",
    "print(\"\\n가공된 질문 문장 =>\",questions[:10])\n",
    "\n",
    "# Tab글자는 Start of Sentence임\n",
    "answers = ['\\t ' + lines for lines in seq[1::2]] \n",
    "print(\"\\n가공된 답변 문장 =>\",answers[:10])\n",
    "\n",
    "num_sample = len(questions)\n",
    "\n",
    "perm = list(range(num_sample))\n",
    "print(\"\\nperm =>\",perm[:10])\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(perm) # 리스트 값을 섞음\n",
    "print(\"\\nperm =>\",perm[:10])\n",
    "\n",
    "train_q = list()\n",
    "train_a = list()\n",
    "test_q = list()\n",
    "test_a = list()\n",
    "\n",
    "# train 데이타와 test 데이타를 분리함\n",
    "for idx, qna in enumerate(zip(questions, answers)):\n",
    "    q, a = qna\n",
    "    if perm[idx] > num_sample//5:  # 5의 몫을 구함. 500//5 -> 100\n",
    "        train_q.append(q)\n",
    "        train_a.append(a)\n",
    "    else:\n",
    "        test_q.append(q)\n",
    "        test_a.append(a)\n",
    "        \n",
    "# NUM_WORDS개의 단어만 토큰으로 저장하고 필터에 있는 문자는 제외하는 설정\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "# 실제로 문장을 받아서 토큰 처리함\n",
    "tokenizer.fit_on_texts(train_q + train_a)\n",
    "\n",
    "# 문장들을 토큰화된 숫자로 치환시킴\n",
    "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
    "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
    "print('\\n질문 문장=>',train_q[:10])\n",
    "print('\\n질문 토큰 문장=>',train_q_seq[:10])\n",
    "print('\\n아메리카노 토큰 =>',tokenizer.word_index['아메리카노']) # 중요한 함수\n",
    "\n",
    "\n",
    "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
    "test_a_seq = tokenizer.texts_to_sequences(test_a)\n",
    "\n",
    "# 문장의 길이를 64개 단어로 구성하도록하며, 문장이 짧을 경우 빈칸을 채워준다.\n",
    "# 출력 문장은 입력 문장에 EOS를 추가하기 위하여 한단어가 더 길게 구성됨\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=64)\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=65)\n",
    "print('\\n질문 패딩 문장=>',x_train[:10])\n",
    "print('\\n답변 패딩 문장=>',y_train[:10])\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n",
    "                                                       value=0,\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=64)\n",
    "y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=65)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 환경 정의\n",
    "### 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# 문장의 시작 토큰과 문장의 종료 토큰을 설정함\n",
    "model = Seq2seq(sos=tokenizer.word_index['\\t'],\n",
    "                eos=tokenizer.word_index['\\n'])\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define performance metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.872729778289795, Accuracy: 83.12577819824219\n",
      "47.24568510055542 seconds elapsed.\n",
      "Epoch 2, Loss: 0.6804295778274536, Accuracy: 90.3900375366211\n",
      "27.35051465034485 seconds elapsed.\n",
      "Epoch 3, Loss: 0.5826170444488525, Accuracy: 90.65241241455078\n",
      "27.272101879119873 seconds elapsed.\n",
      "Epoch 4, Loss: 0.5663137435913086, Accuracy: 91.05967712402344\n",
      "27.64638590812683 seconds elapsed.\n",
      "Epoch 5, Loss: 0.5492140054702759, Accuracy: 91.16932678222656\n",
      "27.615357875823975 seconds elapsed.\n",
      "Epoch 6, Loss: 0.537538468837738, Accuracy: 91.17716217041016\n",
      "27.731791734695435 seconds elapsed.\n",
      "Epoch 7, Loss: 0.520825982093811, Accuracy: 91.16541290283203\n",
      "27.143136739730835 seconds elapsed.\n",
      "Epoch 8, Loss: 0.5313218832015991, Accuracy: 91.23198699951172\n",
      "26.398317098617554 seconds elapsed.\n",
      "Epoch 9, Loss: 0.5190854072570801, Accuracy: 91.23198699951172\n",
      "28.0290367603302 seconds elapsed.\n",
      "Epoch 10, Loss: 0.49917763471603394, Accuracy: 91.37296295166016\n",
      "27.892261028289795 seconds elapsed.\n",
      "Epoch 11, Loss: 0.47935664653778076, Accuracy: 91.6588363647461\n",
      "28.566901206970215 seconds elapsed.\n",
      "Epoch 12, Loss: 0.4662967324256897, Accuracy: 91.99561309814453\n",
      "27.991703987121582 seconds elapsed.\n",
      "Epoch 13, Loss: 0.45871999859809875, Accuracy: 92.21882629394531\n",
      "28.10103702545166 seconds elapsed.\n",
      "Epoch 14, Loss: 0.4485000967979431, Accuracy: 92.34806060791016\n",
      "27.905054330825806 seconds elapsed.\n",
      "Epoch 15, Loss: 0.43800634145736694, Accuracy: 92.46945190429688\n",
      "28.095136165618896 seconds elapsed.\n",
      "Epoch 16, Loss: 0.43711596727371216, Accuracy: 92.5203628540039\n",
      "28.07809281349182 seconds elapsed.\n",
      "Epoch 17, Loss: 0.43239763379096985, Accuracy: 92.54386138916016\n",
      "28.926695585250854 seconds elapsed.\n",
      "Epoch 18, Loss: 0.42282602190971375, Accuracy: 92.66525268554688\n",
      "28.982231378555298 seconds elapsed.\n",
      "Epoch 19, Loss: 0.4169303774833679, Accuracy: 92.64958953857422\n",
      "28.074129581451416 seconds elapsed.\n",
      "Epoch 20, Loss: 0.4117310643196106, Accuracy: 92.73182678222656\n",
      "28.150962829589844 seconds elapsed.\n",
      "Epoch 21, Loss: 0.40637657046318054, Accuracy: 92.8140640258789\n",
      "27.9759202003479 seconds elapsed.\n",
      "Epoch 22, Loss: 0.3980356752872467, Accuracy: 92.82972717285156\n",
      "28.456714868545532 seconds elapsed.\n",
      "Epoch 23, Loss: 0.3988646864891052, Accuracy: 92.84539031982422\n",
      "30.43359637260437 seconds elapsed.\n",
      "Epoch 24, Loss: 0.39470794796943665, Accuracy: 92.86105346679688\n",
      "28.475069284439087 seconds elapsed.\n",
      "Epoch 25, Loss: 0.39037179946899414, Accuracy: 92.9197998046875\n",
      "28.559916257858276 seconds elapsed.\n",
      "Epoch 26, Loss: 0.3817386329174042, Accuracy: 92.92762756347656\n",
      "28.068310499191284 seconds elapsed.\n",
      "Epoch 27, Loss: 0.37907174229621887, Accuracy: 93.01378631591797\n",
      "25.92473316192627 seconds elapsed.\n",
      "Epoch 28, Loss: 0.37626397609710693, Accuracy: 93.03728485107422\n",
      "27.650057077407837 seconds elapsed.\n",
      "Epoch 29, Loss: 0.3682582974433899, Accuracy: 93.07644653320312\n",
      "27.7650728225708 seconds elapsed.\n",
      "Epoch 30, Loss: 0.3658582866191864, Accuracy: 93.19392395019531\n",
      "27.79658555984497 seconds elapsed.\n",
      "Epoch 31, Loss: 0.3574131429195404, Accuracy: 93.1547622680664\n",
      "27.733299732208252 seconds elapsed.\n",
      "Epoch 32, Loss: 0.35698631405830383, Accuracy: 93.26441192626953\n",
      "27.786489248275757 seconds elapsed.\n",
      "Epoch 33, Loss: 0.35419103503227234, Accuracy: 93.22917175292969\n",
      "27.830464601516724 seconds elapsed.\n",
      "Epoch 34, Loss: 0.34490299224853516, Accuracy: 93.28399658203125\n",
      "28.100292444229126 seconds elapsed.\n",
      "Epoch 35, Loss: 0.3427898585796356, Accuracy: 93.29573822021484\n",
      "27.810524463653564 seconds elapsed.\n",
      "Epoch 36, Loss: 0.3400864005088806, Accuracy: 93.32315063476562\n",
      "28.45223832130432 seconds elapsed.\n",
      "Epoch 37, Loss: 0.33018577098846436, Accuracy: 93.3505630493164\n",
      "28.193539142608643 seconds elapsed.\n",
      "Epoch 38, Loss: 0.32698360085487366, Accuracy: 93.42888641357422\n",
      "28.09471583366394 seconds elapsed.\n",
      "Epoch 39, Loss: 0.325752854347229, Accuracy: 93.52678680419922\n",
      "28.441383838653564 seconds elapsed.\n",
      "Epoch 40, Loss: 0.3187224566936493, Accuracy: 93.5150375366211\n",
      "28.361602783203125 seconds elapsed.\n",
      "Epoch 41, Loss: 0.3127509653568268, Accuracy: 93.58552551269531\n",
      "28.26901912689209 seconds elapsed.\n",
      "Epoch 42, Loss: 0.3088022768497467, Accuracy: 93.58161163330078\n",
      "28.316123485565186 seconds elapsed.\n",
      "Epoch 43, Loss: 0.30799049139022827, Accuracy: 93.6442642211914\n",
      "28.329914093017578 seconds elapsed.\n",
      "Epoch 44, Loss: 0.300547331571579, Accuracy: 93.69517517089844\n",
      "28.401415586471558 seconds elapsed.\n",
      "Epoch 45, Loss: 0.2967563271522522, Accuracy: 93.7108383178711\n",
      "28.31386947631836 seconds elapsed.\n",
      "Epoch 46, Loss: 0.2917903661727905, Accuracy: 93.76957702636719\n",
      "28.65853524208069 seconds elapsed.\n",
      "Epoch 47, Loss: 0.2907598614692688, Accuracy: 93.82048797607422\n",
      "28.632872104644775 seconds elapsed.\n",
      "Epoch 48, Loss: 0.28270116448402405, Accuracy: 93.93797302246094\n",
      "27.325490713119507 seconds elapsed.\n",
      "Epoch 49, Loss: 0.2739257216453552, Accuracy: 93.91056060791016\n",
      "29.072739362716675 seconds elapsed.\n",
      "Epoch 50, Loss: 0.2712228000164032, Accuracy: 94.06327819824219\n",
      "27.93855619430542 seconds elapsed.\n",
      "Epoch 51, Loss: 0.26353463530540466, Accuracy: 94.11418914794922\n",
      "28.072911977767944 seconds elapsed.\n",
      "Epoch 52, Loss: 0.25905436277389526, Accuracy: 94.22383880615234\n",
      "27.840856790542603 seconds elapsed.\n",
      "Epoch 53, Loss: 0.2542222738265991, Accuracy: 94.27082824707031\n",
      "27.93598747253418 seconds elapsed.\n",
      "Epoch 54, Loss: 0.2519931495189667, Accuracy: 94.42355346679688\n",
      "28.075113534927368 seconds elapsed.\n",
      "Epoch 55, Loss: 0.2455333173274994, Accuracy: 94.50187683105469\n",
      "28.022080183029175 seconds elapsed.\n",
      "Epoch 56, Loss: 0.24305015802383423, Accuracy: 94.63894653320312\n",
      "28.14586067199707 seconds elapsed.\n",
      "Epoch 57, Loss: 0.23321771621704102, Accuracy: 94.70943450927734\n",
      "28.61297869682312 seconds elapsed.\n",
      "Epoch 58, Loss: 0.23020099103450775, Accuracy: 94.83866119384766\n",
      "28.219109535217285 seconds elapsed.\n",
      "Epoch 59, Loss: 0.22347590327262878, Accuracy: 94.90523529052734\n",
      "28.5867178440094 seconds elapsed.\n",
      "Epoch 60, Loss: 0.2171085625886917, Accuracy: 95.12844848632812\n",
      "28.536845445632935 seconds elapsed.\n",
      "Epoch 61, Loss: 0.21311242878437042, Accuracy: 95.2537612915039\n",
      "28.489527463912964 seconds elapsed.\n",
      "Epoch 62, Loss: 0.21077436208724976, Accuracy: 95.3712387084961\n",
      "28.432337760925293 seconds elapsed.\n",
      "Epoch 63, Loss: 0.20190462470054626, Accuracy: 95.47697448730469\n",
      "28.23842716217041 seconds elapsed.\n",
      "Epoch 64, Loss: 0.19836392998695374, Accuracy: 95.55529022216797\n",
      "28.57367467880249 seconds elapsed.\n",
      "Epoch 65, Loss: 0.1901533305644989, Accuracy: 95.66102600097656\n",
      "28.592644214630127 seconds elapsed.\n",
      "Epoch 66, Loss: 0.18275348842144012, Accuracy: 95.8489990234375\n",
      "28.8999125957489 seconds elapsed.\n",
      "Epoch 67, Loss: 0.17892251908779144, Accuracy: 96.09571075439453\n",
      "28.547073364257812 seconds elapsed.\n",
      "Epoch 68, Loss: 0.17769962549209595, Accuracy: 96.13487243652344\n",
      "27.73529362678528 seconds elapsed.\n",
      "Epoch 69, Loss: 0.16775652766227722, Accuracy: 96.26409912109375\n",
      "27.82208800315857 seconds elapsed.\n",
      "Epoch 70, Loss: 0.16563795506954193, Accuracy: 96.47164916992188\n",
      "27.95519256591797 seconds elapsed.\n",
      "Epoch 71, Loss: 0.15996216237545013, Accuracy: 96.51472473144531\n",
      "27.945197105407715 seconds elapsed.\n",
      "Epoch 72, Loss: 0.15367409586906433, Accuracy: 96.60479736328125\n",
      "28.733166694641113 seconds elapsed.\n",
      "Epoch 73, Loss: 0.14831294119358063, Accuracy: 96.76143646240234\n",
      "28.015440225601196 seconds elapsed.\n",
      "Epoch 74, Loss: 0.14406487345695496, Accuracy: 96.84758758544922\n",
      "28.19517707824707 seconds elapsed.\n",
      "Epoch 75, Loss: 0.13680993020534515, Accuracy: 96.98856353759766\n",
      "28.236868619918823 seconds elapsed.\n",
      "Epoch 76, Loss: 0.1317361444234848, Accuracy: 97.19611358642578\n",
      "28.429370164871216 seconds elapsed.\n",
      "Epoch 77, Loss: 0.12607762217521667, Accuracy: 97.30184936523438\n",
      "28.42588233947754 seconds elapsed.\n",
      "Epoch 78, Loss: 0.12077338248491287, Accuracy: 97.4702377319336\n",
      "28.31665301322937 seconds elapsed.\n",
      "Epoch 79, Loss: 0.11755882948637009, Accuracy: 97.48981475830078\n",
      "28.38603377342224 seconds elapsed.\n",
      "Epoch 80, Loss: 0.11276697367429733, Accuracy: 97.59555053710938\n",
      "28.371449947357178 seconds elapsed.\n",
      "Epoch 81, Loss: 0.10968364030122757, Accuracy: 97.64253997802734\n",
      "29.033292055130005 seconds elapsed.\n",
      "Epoch 82, Loss: 0.10295498371124268, Accuracy: 97.83443450927734\n",
      "28.677396059036255 seconds elapsed.\n",
      "Epoch 83, Loss: 0.09868263453245163, Accuracy: 97.9989013671875\n",
      "29.052712202072144 seconds elapsed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Loss: 0.09395474940538406, Accuracy: 98.01457214355469\n",
      "28.147758960723877 seconds elapsed.\n",
      "Epoch 85, Loss: 0.09000364691019058, Accuracy: 98.2025375366211\n",
      "27.924251317977905 seconds elapsed.\n",
      "Epoch 86, Loss: 0.08669168502092361, Accuracy: 98.27693939208984\n",
      "27.74576449394226 seconds elapsed.\n",
      "Epoch 87, Loss: 0.0829743817448616, Accuracy: 98.36309814453125\n",
      "28.165634870529175 seconds elapsed.\n",
      "Epoch 88, Loss: 0.07889547944068909, Accuracy: 98.46099853515625\n",
      "28.01335859298706 seconds elapsed.\n",
      "Epoch 89, Loss: 0.07541748881340027, Accuracy: 98.54714965820312\n",
      "27.858492612838745 seconds elapsed.\n",
      "Epoch 90, Loss: 0.07086700946092606, Accuracy: 98.60197448730469\n",
      "27.976922035217285 seconds elapsed.\n",
      "Epoch 91, Loss: 0.06849238276481628, Accuracy: 98.75469970703125\n",
      "27.99857449531555 seconds elapsed.\n",
      "Epoch 92, Loss: 0.06606905907392502, Accuracy: 98.80952453613281\n",
      "28.247173070907593 seconds elapsed.\n",
      "Epoch 93, Loss: 0.06238634139299393, Accuracy: 98.90350341796875\n",
      "28.725528717041016 seconds elapsed.\n",
      "Epoch 94, Loss: 0.06120144948363304, Accuracy: 98.91917419433594\n",
      "28.188483238220215 seconds elapsed.\n",
      "Epoch 95, Loss: 0.05900588259100914, Accuracy: 98.98966217041016\n",
      "28.25581431388855 seconds elapsed.\n",
      "Epoch 96, Loss: 0.055202726274728775, Accuracy: 99.02881622314453\n",
      "28.217106580734253 seconds elapsed.\n",
      "Epoch 97, Loss: 0.05302625894546509, Accuracy: 99.09931182861328\n",
      "28.15794801712036 seconds elapsed.\n",
      "Epoch 98, Loss: 0.05018661171197891, Accuracy: 99.11497497558594\n",
      "28.499316453933716 seconds elapsed.\n",
      "Epoch 99, Loss: 0.048213258385658264, Accuracy: 99.1815414428711\n",
      "28.29235863685608 seconds elapsed.\n",
      "Epoch 100, Loss: 0.04610548913478851, Accuracy: 99.24812316894531\n",
      "28.47850489616394 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# 시간측정 필요. 노트북의 경우 1 epoch당 30초 정도 소요됨\n",
    "t = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    for seqs, labels in train_ds:\n",
    "        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100))\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    ct = time.time()\n",
    "    print('{} seconds elapsed.'.format(ct - t))\n",
    "    t = ct       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "질문 :  ['여기 기프티콘 되죠 \\n']\n",
      "답변 :  ['\\t 네 현금영수증 해드릴까 요 \\n']\n",
      "예측 :  ['네 진동 벨 울리면 찾으러 오세요 \\n']\n",
      "_\n",
      "질문 :  ['네 에 테이크 아웃 도 가능한가요 \\n']\n",
      "답변 :  ['\\t 네 로 오시 면 테이크 아웃 잔 에 담아 드려요 \\n']\n",
      "예측 :  ['아뇨 현재 법적 으로 금지 \\n']\n",
      "_\n",
      "질문 :  ['아메리카노 톨 사이즈 로 주세요 \\n']\n",
      "답변 :  ['\\t 따뜻한 거 로 드릴 까요 \\n']\n",
      "예측 :  ['아이스 아메리카노 한잔 주문 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['진동 을 따로 주시나요 \\n']\n",
      "답변 :  ['\\t 주 번호 로 드리겠습니다 \\n']\n",
      "예측 :  ['네 포인트 할인 가능하세요 \\n']\n",
      "_\n",
      "질문 :  ['자리 있나요 \\n']\n",
      "답변 :  ['\\t 네 있습니다 \\n']\n",
      "예측 :  ['네 유효 차는 있습니다 \\n']\n",
      "_\n",
      "질문 :  ['그럼 루이보스 밀크 티 하나 \\n']\n",
      "답변 :  ['\\t 네 알겠습니다 \\n']\n",
      "예측 :  ['네 4500원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['다음 에 무료 로 하고 엔 도장 찍어주세요 \\n']\n",
      "답변 :  ['\\t 네 \\n']\n",
      "예측 :  ['네 알겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['아메리카노 한 잔 에 얼마 죠 \\n']\n",
      "답변 :  ['\\t 입니다 \\n']\n",
      "예측 :  ['4000원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['얼마나 \\n']\n",
      "답변 :  ['\\t 바로 만들어 드릴게요 \\n']\n",
      "예측 :  ['따뜻한 걸 로 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['카푸치노 는 로 주시 고 아메리카노 는 로 \\n']\n",
      "답변 :  ['\\t 네 더 없으세요 \\n']\n",
      "예측 :  ['네 저희 꺼 4500원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['아메리카노 는 어떤 종류 가 있나요 \\n']\n",
      "답변 :  ['\\t 디카 페인 과 기본 아메리카노 2 종류 있습니다 \\n']\n",
      "예측 :  ['네 고객 님 사이즈 는 다 되었습니다 \\n']\n",
      "_\n",
      "질문 :  ['카카오 페이 로 결제 가능한가요 \\n']\n",
      "답변 :  ['\\t 네 가능합니다 \\n']\n",
      "예측 :  ['네 가능하세요 \\n']\n",
      "_\n",
      "질문 :  ['오늘 의 커피 는 커피 로 하나요 맛 이 \\n']\n",
      "답변 :  ['\\t 아 네 오늘 은 과테말라 커피 입니다 \\n']\n",
      "예측 :  ['네 사이즈 는 어떤 걸 로 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['머핀 은 뭐 가 제일 \\n']\n",
      "답변 :  ['\\t 블루베리 머핀 이 잘 나갑니다 \\n']\n",
      "예측 :  ['네 고객 님 포인트 적립 있습니다 \\n']\n",
      "_\n",
      "질문 :  ['현금 영수증 해주세요 \\n']\n",
      "답변 :  ['\\t 네 번호 찍어주세요 \\n']\n",
      "예측 :  ['네 번호 찍어주세요 \\n']\n",
      "_\n",
      "질문 :  ['둘 다 톨 사이즈 로 주세요 \\n']\n",
      "답변 :  ['\\t 여기 서 드시고 요 \\n']\n",
      "예측 :  ['네 어떤 걸 로 알려 드리겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['아이스 아메리카노 한 잔 가능한가요 \\n']\n",
      "답변 :  ['\\t 네 가능합니다 \\n']\n",
      "예측 :  ['네 잠시 만 기다려주세요 \\n']\n",
      "_\n",
      "질문 :  ['아이스 아메리카노 에 샷 이 몇 개 \\n']\n",
      "답변 :  ['\\t 아이스 아메리카노 에 샷 은 개 \\n']\n",
      "예측 :  ['네 아이스 아메리카노 로 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['카페라테 한 잔 주세요 \\n']\n",
      "답변 :  ['\\t 카페라테 따뜻한 걸 로 드릴 까요 \\n']\n",
      "예측 :  ['네 드시고 가시나요 \\n']\n",
      "_\n",
      "질문 :  ['아니요 \\n']\n",
      "답변 :  ['\\t 네 더 필요하신 건 없으신 가요 \\n']\n",
      "예측 :  ['따뜻한 거 맞으세요 \\n']\n",
      "_\n",
      "질문 :  ['네 찍어주세요 \\n']\n",
      "답변 :  ['\\t 네 주문 딸기 스무디 와 쿠키 드릴게요 \\n']\n",
      "예측 :  ['따뜻한 거 맞으세요 \\n']\n",
      "_\n",
      "질문 :  ['시즌 메뉴 오늘 도 가능한가요 \\n']\n",
      "답변 :  ['\\t 네 시즌 메뉴 가능합니다 \\n']\n",
      "예측 :  ['네 고객 님 에서는 있습니다 \\n']\n",
      "_\n",
      "질문 :  ['시즌 메뉴 와 함께 되어 있는 세트 메뉴 가 있나요 \\n']\n",
      "답변 :  ['\\t 네 치즈 케이크 와 시즌 메뉴 두 잔 으로 세트 메뉴 있습니다 \\n']\n",
      "예측 :  ['네 일회용 컵 많이 담아 드릴게요 \\n']\n",
      "_\n",
      "질문 :  ['라테 에 우유 두 도 변경 가능한가요 \\n']\n",
      "답변 :  ['\\t 네 라테 에 두유 로 변경 가능합니다 \\n']\n",
      "예측 :  ['네 배달 비 3000원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['네 먹고 갈 거 예요 \\n']\n",
      "답변 :  ['\\t 카드 여기 주세요 \\n']\n",
      "예측 :  ['포크 는 몇 개 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['카페인 이 음료 있나요 \\n']\n",
      "답변 :  ['\\t 티 음료 와 스무디 에는 카페인 이 않습니다 \\n']\n",
      "예측 :  ['네 가능하세요 \\n']\n",
      "_\n",
      "질문 :  ['딸기스무디 랑 키위 스무디 는 생 과일 인가요 \\n']\n",
      "답변 :  ['\\t 딸기 는 키위 는 생 과일 을 사용 하고 있습니다 \\n']\n",
      "예측 :  ['아이스 가 4000원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['그럼 딸기 스무디 하나 주세요 \\n']\n",
      "답변 :  ['\\t 드시고 가시나요 \\n']\n",
      "예측 :  ['드시고 가세 요 \\n']\n",
      "_\n",
      "질문 :  ['아메리카노 한 잔이요 \\n']\n",
      "답변 :  ['\\t 아이스 아메리카노 로 드릴 까요 \\n']\n",
      "예측 :  ['드시고 가시나요 \\n']\n",
      "_\n",
      "질문 :  ['네 도 같이 \\n']\n",
      "답변 :  ['\\t 네 아메리카노 4000원 입니다 \\n']\n",
      "예측 :  ['네 더 필요한 거 없으신 가요 \\n']\n",
      "_\n",
      "질문 :  ['디카 페인 아이스 아메리카노 한 잔 주세요 \\n']\n",
      "답변 :  ['\\t 디카 페인 아이스 아메리카노 는 기존 금액 에 300원 추가 되는데 괜찮으신 가요 \\n']\n",
      "예측 :  ['네 더 필요하신 건 없으신 가요 \\n']\n",
      "_\n",
      "질문 :  ['커피 음료 것 뭐 가 있나요 \\n']\n",
      "답변 :  ['\\t 스무디 와 주스 있습니다 \\n']\n",
      "예측 :  ['네 됩니다 \\n']\n",
      "_\n",
      "질문 :  ['주스 어떤 종류 있나요 \\n']\n",
      "답변 :  ['\\t 딸기 주스 주스 주스 가 있습니다 \\n']\n",
      "예측 :  ['네 맛있게 드세요 \\n']\n",
      "_\n",
      "질문 :  ['플랫 화이트 라지 로 주세요 \\n']\n",
      "답변 :  ['\\t 네 \\n']\n",
      "예측 :  ['드시고 가시나요 \\n']\n",
      "_\n",
      "질문 :  ['네 레드 벨벳 케이크 주세요 \\n']\n",
      "답변 :  ['\\t 음료 는 뭘 로 드릴 까요 \\n']\n",
      "예측 :  ['따뜻한 걸 로 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['네 먹고 갈 거 예요 \\n']\n",
      "답변 :  ['\\t 유리잔 괜찮으세요 \\n']\n",
      "예측 :  ['포크 는 몇 개 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['따뜻한 밀크 티 주세요 \\n']\n",
      "답변 :  ['\\t 네 \\n']\n",
      "예측 :  ['총 4500원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['음료 얼마나 하나요 \\n']\n",
      "답변 :  ['\\t 10분 정도 주시 면 됩니다 \\n']\n",
      "예측 :  ['네 카드 도 와 드릴게요 \\n']\n",
      "_\n",
      "질문 :  ['아이스 아메리카노 한잔 얼마 인가요 \\n']\n",
      "답변 :  ['\\t 4500원 입니다 \\n']\n",
      "예측 :  ['5천 원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['현금영수증 번호 \\n']\n",
      "답변 :  ['\\t 네 \\n']\n",
      "예측 :  ['네 번호 찍어주세요 \\n']\n",
      "_\n",
      "질문 :  ['이 카드 로 결제 해주세요 \\n']\n",
      "답변 :  ['\\t 네 결제 도 와 드릴게요 \\n']\n",
      "예측 :  ['네 알겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['주문 한 게 다 안 \\n']\n",
      "답변 :  ['\\t 주 번호 가 몇 이 죠 \\n']\n",
      "예측 :  ['네 알겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['을 \\n']\n",
      "답변 :  ['\\t \\n']\n",
      "예측 :  ['500 원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['베이글 은 얼마 인가요 \\n']\n",
      "답변 :  ['\\t 베이글 은 2000원 입니다 \\n']\n",
      "예측 :  ['5천 원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['지금 되나요 \\n']\n",
      "답변 :  ['\\t 는 계절 메뉴 라 지금 은 판매 하지 않습니다 \\n']\n",
      "예측 :  ['네 있습니다 \\n']\n",
      "_\n",
      "질문 :  ['바닐라 라테 는 따뜻하게 주세요 \\n']\n",
      "답변 :  ['\\t 네 적립 이나 할인 카드 있으세요 \\n']\n",
      "예측 :  ['네 그럼 시럽 거 넣어 드릴게요 \\n']\n",
      "_\n",
      "질문 :  ['테이크 아웃 으로 부탁드립니다 \\n']\n",
      "답변 :  ['\\t 결제 는 이 쪽 에서 도 와 드릴게요 \\n']\n",
      "예측 :  ['플랫 화이트 는 이 보다 우유 양 이 좀 적게 들어갔어요 \\n']\n",
      "_\n",
      "질문 :  ['혹시 테이크 아웃 잔 에 수 있나요 \\n']\n",
      "답변 :  ['\\t 테이크 아웃 하시는 건가 요 \\n']\n",
      "예측 :  ['네 저기 있습니다 \\n']\n",
      "_\n",
      "질문 :  ['아메리카노 하나 는 샷 추가 해주세요 \\n']\n",
      "답변 :  ['\\t 아메리카노 는 둘 다 따뜻한 걸 로 드릴 까요 \\n']\n",
      "예측 :  ['네 담아 드립니다 \\n']\n",
      "_\n",
      "질문 :  ['쿠폰 찍어주세요 \\n']\n",
      "답변 :  ['\\t 네 찍어 드릴게요 \\n']\n",
      "예측 :  ['네 이리 찍어주세요 \\n']\n",
      "_\n",
      "질문 :  ['주문 할게요 \\n']\n",
      "답변 :  ['\\t 어떤 거 드릴 까요 \\n']\n",
      "예측 :  ['네 시럽 넣어 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['파나요 \\n']\n",
      "답변 :  ['\\t 는 계절 지금 은 \\n']\n",
      "예측 :  ['따뜻한 거 맞으세요 \\n']\n",
      "_\n",
      "질문 :  ['그럼 겨울 메뉴 뭐 가 \\n']\n",
      "답변 :  ['\\t 겨울 엔 감귤 라테 가 제일 많이 나가요 \\n']\n",
      "예측 :  ['네 저기 에 담아 드리겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['네 결제 는 카드 로 할게요 \\n']\n",
      "답변 :  ['\\t 네 결제 완료 되었습니다 \\n']\n",
      "예측 :  ['네 계산 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['둘 다 사이즈 로 할게요 \\n']\n",
      "답변 :  ['\\t 네 결제 는 어떤 것 으로 도 와 드릴 까요 \\n']\n",
      "예측 :  ['네 총 9500원 에 만 에 와 드리겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['기프티콘 으로 결제 할게요 \\n']\n",
      "답변 :  ['\\t 네 그럼 쿠폰 저 \\n']\n",
      "예측 :  ['저 한테 보여주시고 제 가 확인 버튼 누르면 돼요 \\n']\n",
      "_\n",
      "질문 :  ['녹차 라테 1 잔 주세요 \\n']\n",
      "답변 :  ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n",
      "예측 :  ['휘핑크림 올려 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['네 그럼 휘핑크림 추가 해서 주세요 \\n']\n",
      "답변 :  ['\\t 네 녹차 라테 에 휘핑크림 추가 해서 4500원 입니다 \\n']\n",
      "예측 :  ['네 저희 추가 하실 것 은 없으신 가요 \\n']\n",
      "_\n",
      "질문 :  ['브레드 종류 는 뭐 가 있나요 \\n']\n",
      "답변 :  ['\\t 허니 브레드 와 갈릭 치즈 브레드 가 있습니다 \\n']\n",
      "예측 :  ['네 고객 님 티 종류 는 아이스 가능합니다 \\n']\n",
      "_\n",
      "질문 :  ['생크림 이 건 어떤 건가 요 \\n']\n",
      "답변 :  ['\\t 허니 브레드 입니다 \\n']\n",
      "예측 :  ['네 진동 벨 로 알려 드리겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['네 그렇게 만들어 주세요 \\n']\n",
      "답변 :  ['\\t 더 필요한 건 없으세요 \\n']\n",
      "예측 :  ['네 드시고 가시나요 \\n']\n",
      "_\n",
      "질문 :  ['여기 있습니다 \\n']\n",
      "답변 :  ['\\t 네 확인 되셨고 되면 진동 벨 거 예요 \\n']\n",
      "예측 :  ['네 번호 적어주세요 \\n']\n",
      "_\n",
      "질문 :  ['핫초코 한 잔 아메리카노 사이 즈 업 한 잔 하면 얼마 인가요 \\n']\n",
      "답변 :  ['\\t 입니다 \\n']\n",
      "예측 :  ['500원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['주스 는 다른 건 없나요 \\n']\n",
      "답변 :  ['\\t 그럼 에 라테 추천 \\n']\n",
      "예측 :  ['루이보스 는 는 루이보스 는 다 다 가능합니다 \\n']\n",
      "_\n",
      "질문 :  ['그건 \\n']\n",
      "답변 :  ['\\t 네 만 따듯 해 요 \\n']\n",
      "예측 :  ['따뜻한 거 맞으세요 \\n']\n",
      "_\n",
      "질문 :  ['통신사 할인 되죠 \\n']\n",
      "답변 :  ['\\t 네 300원 할인 됩니다 \\n']\n",
      "예측 :  ['네 번호 입력 해주세요 \\n']\n",
      "_\n",
      "질문 :  ['매장 에서 언제 까지 영업 하시나요 \\n']\n",
      "답변 :  ['\\t 오후 10시 까지 영업 입니다 \\n']\n",
      "예측 :  ['9시 2시 까지 입니다 \\n']\n",
      "_\n",
      "질문 :  ['아니요 그냥 주세요 \\n']\n",
      "답변 :  ['\\t 결제 해드릴게요 \\n']\n",
      "예측 :  ['카페모카 5천 원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['가격 안 되나요 \\n']\n",
      "답변 :  ['\\t 한 해드릴게요 \\n']\n",
      "예측 :  ['네 가능합니다 \\n']\n",
      "_\n",
      "질문 :  ['카페라테 한잔 주세요 \\n']\n",
      "답변 :  ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n",
      "예측 :  ['네 드시고 가시나요 \\n']\n",
      "_\n",
      "질문 :  ['네 차가운 걸 로 주세요 \\n']\n",
      "답변 :  ['\\t 4500원 입니다 \\n']\n",
      "예측 :  ['사이즈 는 어떻게 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['어떤 게 괜찮아요 \\n']\n",
      "답변 :  ['\\t 이나 원두 를 하시는 게 아니면 예 가체 많이 추천 \\n']\n",
      "예측 :  ['아뇨 현재 법적 으로 금지 하고 됩니다 \\n']\n",
      "_\n",
      "질문 :  ['그럼 추천 치즈 케이크 도 같이 주세요 \\n']\n",
      "답변 :  ['\\t 네 매장 에서 드시고 가시나요 \\n']\n",
      "예측 :  ['네 더 필요한 거 없으신 가요 \\n']\n",
      "_\n",
      "질문 :  ['그리고 휘핑크림 은 에스프레소 크림 으로 \\n']\n",
      "답변 :  ['\\t 결제 는 어떻게 해드릴까 요 \\n']\n",
      "예측 :  ['네 어떤 걸 로 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['지금 도 할인 하나요 \\n']\n",
      "답변 :  ['\\t 네 10시 까지 하고 있습니다 \\n']\n",
      "예측 :  ['네 이리 적어주세요 \\n']\n",
      "_\n",
      "질문 :  ['그럼 와 아이스 아메리카노 로 할게요 \\n']\n",
      "답변 :  ['\\t 더 필요하신 건 없나요 \\n']\n",
      "예측 :  ['네 총 18000원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['네 할인 적립 은 \\n']\n",
      "답변 :  ['\\t 네 바코드 \\n']\n",
      "예측 :  ['네 그럼 할인 에 드릴게요 \\n']\n",
      "_\n",
      "질문 :  ['초코 프라푸치노 주세요 \\n']\n",
      "답변 :  ['\\t 휘핑 올려 드릴 까요 \\n']\n",
      "예측 :  ['아이스 프라푸치노 사이즈 는 어떻게 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['시럽 도 가 \\n']\n",
      "답변 :  ['\\t 드시고 가시나요 \\n']\n",
      "예측 :  ['네 맛있게 드세요 \\n']\n",
      "_\n",
      "질문 :  ['둘 다 사이즈 로 주세요 \\n']\n",
      "답변 :  ['\\t 드시고 가시나요 \\n']\n",
      "예측 :  ['네 사이즈 는 어떤 로 하시겠어요 \\n']\n",
      "_\n",
      "질문 :  ['마시다가 갈 건데 테이크아웃 으로 주세요 \\n']\n",
      "답변 :  ['\\t 상 매장 에서는 머그컵 으로 드리고 있어요 \\n']\n",
      "예측 :  ['네 준비 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['나갈 때 테이크아웃 컵 으로 수 있나요 \\n']\n",
      "답변 :  ['\\t 네 그렇게 해드릴게요 \\n']\n",
      "예측 :  ['네 치즈케이크 는 지금 다 팔렸습니다 \\n']\n",
      "_\n",
      "질문 :  ['아메리카노 두 잔 한잔 주세요 \\n']\n",
      "답변 :  ['\\t 드시고 가실 건가 요 \\n']\n",
      "예측 :  ['드시고 가시나요 \\n']\n",
      "_\n",
      "질문 :  ['얼마나 하나요 \\n']\n",
      "답변 :  ['\\t 5분 정도 \\n']\n",
      "예측 :  ['따뜻한 거 로 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['포인트 적립 해주세요 \\n']\n",
      "답변 :  ['\\t 네 번호 입력 부탁드립니다 \\n']\n",
      "예측 :  ['네 4500원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['네 번호 로 할게요 \\n']\n",
      "답변 :  ['\\t 네 에 번호 \\n']\n",
      "예측 :  ['네 번호 입력 해주세요 \\n']\n",
      "_\n",
      "질문 :  ['아 포인트 포인트 사용 해주세요 \\n']\n",
      "답변 :  ['\\t 네 고객 님 포인트 총 있으신 데 사용 도 와 드리겠습니다 \\n']\n",
      "예측 :  ['네 포인트 적립 해드릴게요 \\n']\n",
      "_\n",
      "질문 :  ['톨 사이즈 로 주문 할게요 \\n']\n",
      "답변 :  ['\\t 네 계산 도 와 드리겠습니다 \\n']\n",
      "예측 :  ['네 아이스 아메리카노 어떤 걸 로 주문 주문 도 음료 말씀 주문 음료 음료 가져가세요 \\n']\n",
      "_\n",
      "질문 :  ['아메리카노 사이즈 가능한가요 \\n']\n",
      "답변 :  ['\\t 네 500원 만 추가 하시면 가능하십니다 \\n']\n",
      "예측 :  ['네 치즈케이크 는 지금 없어요 \\n']\n",
      "_\n",
      "질문 :  ['사이 즈 업 해서 주세요 \\n']\n",
      "답변 :  ['\\t 네 결제 는 어떻게 도 와 드릴 까요 \\n']\n",
      "예측 :  ['아이스 프라푸치노 시럽 은 은 아이스 로 드릴 까요 \\n']\n",
      "_\n",
      "질문 :  ['커피 는 텀블러 에 담아주세요 \\n']\n",
      "답변 :  ['\\t 네 텀블러 할인 4000원 결제 도 와 드리겠습니다 \\n']\n",
      "예측 :  ['텀블러 사용 300원 같이 해드릴게요 \\n']\n",
      "_\n",
      "질문 :  ['아니요 아이스 로 주세요 \\n']\n",
      "답변 :  ['\\t 드시고 가실 건가 요 \\n']\n",
      "예측 :  ['드시고 가시나요 \\n']\n",
      "_\n",
      "질문 :  ['테이크아웃 할게요 \\n']\n",
      "답변 :  ['\\t 지금 중 인데 케이크 주문 하시면 아메리카노 한잔 로 드려요 \\n']\n",
      "예측 :  ['네 알겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['현금 결제 가 안 \\n']\n",
      "답변 :  ['\\t 현금 은 에서 주문 도 와 드리겠습니다 \\n']\n",
      "예측 :  ['네 담아 드립니다 \\n']\n",
      "_\n",
      "질문 :  ['포인트 적립 되나요 \\n']\n",
      "답변 :  ['\\t 번호 포인트 적립 도 와 드리고 있어요 \\n']\n",
      "예측 :  ['네 \\n']\n",
      "_\n",
      "질문 :  ['포인트 적립 할게요 \\n']\n",
      "답변 :  ['\\t 네 결제 되셨습니다 \\n']\n",
      "예측 :  ['네 카드 받았습니다 \\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "질문 :  ['티라미수 는 있나요 \\n']\n",
      "답변 :  ['\\t 네 티라미수 는 있습니다 \\n']\n",
      "예측 :  ['네 그럼요 \\n']\n",
      "_\n",
      "질문 :  ['네 현금영수증 해주세요 \\n']\n",
      "답변 :  ['\\t 네 드시고 가시나요 \\n']\n",
      "예측 :  ['네 번호 찍어주세요 \\n']\n",
      "_\n",
      "질문 :  ['샷 추가 해주세요 \\n']\n",
      "답변 :  ['\\t 네 알겠습니다 \\n']\n",
      "예측 :  ['네 앞 에 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "질문 :  ['얼마 에요 \\n']\n",
      "답변 :  ['\\t 만 원 입니다 \\n']\n",
      "예측 :  ['베이글 과 동일하게 2000원 입니다 \\n']\n",
      "_\n",
      "질문 :  ['아이스 아메리카노 랑 샌드위치 주세요 \\n']\n",
      "답변 :  ['\\t 10시 에 세트 할인 가능하세요 \\n']\n",
      "예측 :  ['아이스 아메리카노 안 안 입니다 \\n']\n"
     ]
    }
   ],
   "source": [
    "for test_seq, test_labels in test_ds:\n",
    "    prediction = test_step(model, test_seq)\n",
    "    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n",
    "    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n",
    "    texts = tokenizer.sequences_to_texts(prediction.numpy())\n",
    "    print('_')\n",
    "    print('질문 : ', test_text)\n",
    "    print('답변 : ', gt_text)\n",
    "    print('예측 : ', texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
